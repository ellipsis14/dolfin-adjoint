#!/usr/bin/env python

# Copyright (C) 2011-2012 by Imperial College London
# Copyright (C) 2013 University of Oxford
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Lesser General Public License as published by
# the Free Software Foundation, version 3 of the License
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

# Based on burgers_newton.py test from dolfin-adjoint
# Code first added: 2012-10-25

from dolfin import *
from dolfin_adjoint_timestepping import *

import numpy

if "reorder_dofs_serial" in parameters:
  parameters["reorder_dofs_serial"] = False

ngrid = 30
nu = StaticConstant(0.0001)
dt = StaticConstant(0.05 / ngrid)
t_end = 0.2

mesh = UnitIntervalMesh(ngrid)
space = FunctionSpace(mesh, "CG", 2)
test, trial = TestFunction(space), TrialFunction(space)

ic = StaticFunction(space, name = "initial_condition")
ic.assign(project(Expression("sin(2.0 * pi * x[0])"), space))

system = TimeSystem()
levels = TimeLevels(levels = [n, n + 1], cycle_map = {n:n + 1})
u = TimeFunction(levels, space, name = "u")
system.add_solve(ic, u[0])
system.add_solve(inner(test, trial) * dx == (inner(test, u[n]) - dt * (inner(test, dot(as_vector([u[n]]), grad(u[n]))) + nu * inner(grad(test), grad(u[n])))) * dx,
  u[n + 1],
  StaticDirichletBC(space, 0.0, "on_boundary"), solver_parameters = {"linear_solver":"lu"})

system = system.assemble(adjoint = True, disk_period = 10)
t = 0.0
while t <= t_end:
  system.timestep()
  t += float(dt)
system.finalise()
parameters["adjoint"]["stop_annotating"] = True

dolfin_adjoint_solution = numpy.array(map(float, """0.00000000e+00   9.16019591e-02   1.83344371e-01   2.73655173e-01
   3.62085710e-01   4.46361854e-01   5.22952008e-01   5.85781576e-01
   6.26980202e-01   6.38974037e-01   6.17952448e-01   5.65980079e-01
   4.89129248e-01   3.90933758e-01   2.59629427e-01  -4.58264304e-10
  -2.59629427e-01  -3.90933761e-01  -4.89129230e-01  -5.65980071e-01
  -6.17952368e-01  -6.38973973e-01  -6.26979993e-01  -5.85781432e-01
  -5.22952200e-01  -4.46362706e-01  -3.62085541e-01  -2.73655285e-01
  -1.83345079e-01  -9.16015364e-02   0.00000000e+00   4.65226676e-02
   1.38897349e-01   2.30965939e-01   3.22458963e-01   4.13369101e-01
   5.04423803e-01   5.97094168e-01   6.93967864e-01   7.97755504e-01
   9.09635104e-01   1.02719895e+00   1.14375157e+00   1.24911373e+00
   1.32992945e+00   1.36392273e+00  -1.36392273e+00  -1.32992945e+00
  -1.24911372e+00  -1.14375156e+00  -1.02719889e+00  -9.09635046e-01
  -7.97755343e-01  -6.93967668e-01  -5.97094098e-01  -5.04424409e-01
  -4.13369567e-01  -3.22458618e-01  -2.30966635e-01  -1.38897326e-01
  -4.65223388e-02""".split()))
err = abs(u[N].vector().array() - dolfin_adjoint_solution).max()
print u[N].vector().array(), err
assert(err < 5.0e-9)

system.verify_checkpoints()

system.set_functional(u[N] * u[N] * dx)
J = system.compute_functional()
grad = system.compute_gradient([ic, nu])
dolfin_adjoint_grad = numpy.array(map(float, """4.35808910e-06   4.60253704e-03   9.01338616e-03   1.30478448e-02
   1.65350220e-02   1.93224140e-02   2.12881373e-02   2.23949812e-02
   2.22379601e-02   2.20663596e-02   1.87575542e-02   1.34295448e-02
   8.39531608e-03   4.67044807e-03   2.10836167e-03   8.34222494e-11
  -2.10836167e-03  -4.67044814e-03  -8.39531618e-03  -1.34295450e-02
  -1.87575549e-02  -2.20663593e-02  -2.22379586e-02  -2.23949832e-02
  -2.12881398e-02  -1.93224112e-02  -1.65350122e-02  -1.30478323e-02
  -9.01339892e-03  -4.60256929e-03  -4.35065042e-06   4.61189634e-03
   1.36433853e-02   2.21058079e-02   2.96427327e-02   3.59324175e-02
   4.07041172e-02   4.37127776e-02   4.48751697e-02   4.41279068e-02
   4.14083285e-02   3.21424684e-02   2.17426589e-02   1.29335505e-02
   6.71895129e-03   2.15811865e-03  -2.15811869e-03  -6.71895125e-03
  -1.29335505e-02  -2.17426587e-02  -3.21424680e-02  -4.14083278e-02
  -4.41279083e-02  -4.48751697e-02  -4.37127740e-02  -4.07041146e-02
  -3.59324138e-02  -2.96427211e-02  -2.21058634e-02  -1.36433869e-02
  -4.61188534e-03""".split()))
err = abs(grad[0].array() - dolfin_adjoint_grad).max()
print grad[0].array(), err
assert(err < 5.0e-11)

orders = system.taylor_test(ic, J = J, grad = grad[0])
assert((orders > 2.0).all())

orders = system.taylor_test(nu, J = J, grad = grad[1])
assert((orders > 1.99).all())

assert(replay_dolfin(forget = False, tol = 0.0, stop = True))

grad_da = compute_gradient(Functional(u[N] * u[N] * dx * dolfin_adjoint.dt[FINISH_TIME]), [InitialConditionParameter(ic), ScalarParameter(nu)])

err = (grad[0] - grad_da[0].vector()).norm("linf")
print "%s %.17e" % (grad_da[0].vector().array(), err)
assert(err < 1.0e-16)

err = abs(float(grad[1]) - grad_da[1])
print "%.17e %.17e" % (grad_da[1], err)
assert(err < 8.0e-15)